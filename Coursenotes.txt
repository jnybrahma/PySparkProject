%----------------------------------------- Section-1 Understanding Big Data and Data Lake ---------------------------------%
## Overview

- Sparks is Data processing engine


## Big Data and How it Started

- Big Data problem to handle;
1. Variety (Structured, Semi-Structured, Un-Structured)
2. Volume (Size)
3. Velocity (Rate of Data generation) 



## Hadoop Architecture , History and Evolution




## Data Lake and How it works


## Apache Spark and Databricks Cloud



%----------------------------------------- Section-2 Installing and Using Apache Spark ------------------------------------%



C:\> conda create --name ApackeSparkENV python=3.9

# To activate this environment, use
#
#     $ conda activate ApackeSparkENV
#
# To deactivate an active environment, use
#
#     $ conda deactivate


C:\>conda activate ApackeSparkENV

(ApackeSparkENV) C:\> pip install jupyterlab
(ApackeSparkENV) C:\> jupyter-lab --notebook-dir=C:/

## Create python environment

C:\Users\Your Name> python -m venv ApackeSparkENV_3.9

C:\>cd  ApackeSparkENV_3.9

C:\ApackeSparkENV_3.9>dir
 Volume in drive C is OS
 Volume Serial Number is 8A67-CB24

 Directory of C:\ApackeSparkENV_3.9

22/06/2025  10:40 AM    <DIR>          .
22/06/2025  10:40 AM    <DIR>          Include
22/06/2025  10:40 AM    <DIR>          Lib
22/06/2025  10:40 AM               117 pyvenv.cfg
22/06/2025  10:40 AM    <DIR>          Scripts
               1 File(s)            117 bytes
               4 Dir(s)  336,432,709,632 bytes free

C:\ApackeSparkENV_3.9>Scripts\activate

(ApackeSparkENV_3.9) C:\ApackeSparkENV_3.9>


(ApackeSparkENV_3.9) C:\> pip install uv

(ApackeSparkENV_3.9) C:\> uv sync



C:\ApackeSparkENV_3.9> .\.venv\Scripts\activate

(ApackeSparkENV_3.9) C:\ApackeSparkENV_3.9> uv run jupyter-lab --notebook-dir=C:/

(ApackeSparkENV_3.9) C:\ApackeSparkENV_3.9> uv pip list



## Spark Development Environment
- Cloud Platforms (Databrick)
- On-Premise Platforms(Local Computer)

## Databricks free community edition;

- Create account and signup in free edition;

https://www.databricks.com/learn/free-edition

-----------------------------------------------------------------------------------------------------------------

C:\Users\User>conda env list

# conda environments:
#
base                   C:\Users\User\anaconda3
AIAgentEnv             C:\Users\User\anaconda3\envs\AIAgentEnv
BigDataML              C:\Users\User\anaconda3\envs\BigDataML
FirstSparkDemo         C:\Users\User\anaconda3\envs\FirstSparkDemo
GenAIEnv               C:\Users\User\anaconda3\envs\GenAIEnv
MLDeploymentEnv        C:\Users\User\anaconda3\envs\MLDeploymentEnv


C:\Users\User> conda activate FirstSparkDemo

(FirstSparkDemo) C:\Users\User> pip list
Package    Version
---------- -------
colorama   0.4.6
iniconfig  2.1.0
numpy      2.3.1
packaging  25.0
pip        25.1
pluggy     1.6.0
Pygments   2.19.2
pyspark    3.4.4
pytest     8.4.1
setuptools 78.1.1
wheel      0.45.1

-----------------------------------------------------------------------------------------------

(FirstSparkDemo) C:\Users\User> pip list
Package                 Version
----------------------- -----------
asttokens               3.0.0
colorama                0.4.6
comm                    0.2.3
debugpy                 1.8.16
decorator               5.2.1
exceptiongroup          1.3.0
executing               2.2.1
findspark               2.0.1
importlib_metadata      8.7.0
iniconfig               2.1.0
ipykernel               6.30.1
ipython                 9.5.0
ipython_pygments_lexers 1.1.1
jedi                    0.19.2
jupyter_client          8.6.3
jupyter_core            5.8.1
matplotlib-inline       0.1.7
nest_asyncio            1.6.0
numpy                   2.3.1
packaging               25.0
parso                   0.8.5
pickleshare             0.7.5
pip                     25.1
platformdirs            4.4.0
pluggy                  1.6.0
prompt_toolkit          3.0.52
psutil                  7.0.0
pure_eval               0.2.3
Pygments                2.19.2
pyspark                 3.4.4
pytest                  8.4.1
python-dateutil         2.9.0.post0
pywin32                 311
pyzmq                   26.2.0
setuptools              78.1.1
six                     1.17.0
stack_data              0.6.3
tornado                 6.5.1
traitlets               5.14.3
typing_extensions       4.15.0
wcwidth                 0.2.13
wheel                   0.45.1
zipp                    3.23.0

(FirstSparkDemo) C:\Users\User>

%-------------------------------------------------------------------------------------%
## Project python 3.9. environment  .venv used in project. 

C:\>cd C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts

C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts> 

C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts> activate

(.venv) C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts>

(.venv) C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts>pip list
Package         Version
--------------- -----------
findspark       2.0.1
grpcio          1.74.0
numpy           2.0.2
pandas          2.3.2
pip             25.0.1
py4j            0.10.9.9
pyarrow         21.0.0
pyspark         3.4.4
python-dateutil 2.9.0.post0
pytz            2025.2
setuptools      80.3.1
six             1.17.0
tzdata          2025.2


(.venv) C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts> python --version
Python 3.9.13

(.venv) C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts> 
(.venv) C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts> pip install findspark
Collecting findspark
  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)
Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)
Installing collected packages: findspark
Successfully installed findspark-2.0.1

[notice] A new release of pip is available: 25.0.1 -> 25.2
[notice] To update, run: python.exe -m pip install --upgrade pip

(.venv) C:\Udemy_Course\ApacheSparkePython\Projects\.venv\Scripts>



%----------------------------- Databricks Sample Example ----------------%
df = (spark.read 
      .formatt("csv")
      .option("header", "true")
      .option("inferSchema", "true") 
      .load("your-file-location/file-name"))

display(df)


--------------------------------------------------------------------------------------------------------------------
Ideally you want to create a managed volume first:

catalog = 'main'
schema = 'default'
volume_name = 'vol1'
spark.sql(f"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume_name}")
Then upload your dataset to this Volume. Then create a table from the volume like so:



file_name = 'your-file-name.csv'
df = spark.read.format('csv').option('inferSchema', True).option('header', True).load(f'/Volumes/{catalog}/{schema}/{volume_name}/{file_name}')

display(df.head(5))


-----------------------------------------------------------------------------------------------------------------------
 Note Path: /Volumes/workspace/default/mydatavolume/boston_house_prices.csv


file_name = 'boston_house_prices.csv'

df = spark.read.format('csv').option('inferSchema', True).option('header', True).load(f'/Volumes/workspace/default/mydatavolume/boston_house_prices.csv')

display(df.head(5))

---------------------------------------------------------------------------------------------------------------------------

Note Path: /Volumes/workspace/default/mydatavolume/employee_salary.csv


file_name = 'employee_salary.csv'

df = spark.read.format('csv').option('inferSchema', True).option('header', True).load(f'/Volumes/workspace/default/mydatavolume/employee_salary.csv')

display(df)

---------------------------------------------------------------

Note: Sample Course Source Codes;

https://github.com/LearningJournal/Spark-Programming-In-Python


%---------------------------------- Spark in Local Machine environment ---------------------------------------%


## Installing and setting  Hadoop WinUtils

https://github.com/cdarlint/winutils

## how to use
## place a copy of hadoop-ver folder on your local drive set environment vars:
## create new folder in C drive Hadoop and copy hadoop-3.3.6

HADOOP_HOME=<your local hadoop-ver folder>
PATH=%PATH%;%HADOOP_HOME%\bin

C:\>setx HADOOP_HOME "C:\Hadoop\hadoop-3.3.6"

SUCCESS: Specified value was saved.

C:\>setx PATH "%PATH%;%HADOOP_HOME%\bin"

SUCCESS: Specified value was saved.

%---------------------------------------- Apache Spark Binaries ----------------------------------------%

## Installatin and Setup

https://spark.apache.org/downloads.html

## Download, extract, rename the folder "spark-3.4.4-bin-hadoop3" to "spark-3.4.4"
## save the folder in "C:\Sparks\"

C:\Sparks\spark-3.4.4


C:\>setx SPARK_HOME "C:\Sparks\spark-3.4.4"

SUCCESS: Specified value was saved.

C:\>setx PATH "%PATH%;%SPARK_HOME%\bin"

SUCCESS: Specified value was saved.

## Now run the pyspark ;

C:\Users\User> pyspark
Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 19:14:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.4
      /_/

Using Python version 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022 16:36:42)
Spark context Web UI available at http://192.168.1.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1752916477512).
SparkSession available as 'spark'.


http://192.168.1.12:4040/jobs/

C:\Sparks\spark-3.4.4\python\lib

## Now Set python path also ;

C:\>setx PYTHONPATH "C:\Sparks\spark-3.4.4\python;C:\Sparks\spark-3.4.4\python\lib\py4j-0.10.9.7-src.zip"

SUCCESS: Specified value was saved.

C:\> where python
C:\Users\User\AppData\Local\Programs\Python\Python39\python.exe
C:\Users\User\AppData\Local\Microsoft\WindowsApps\python.exe

C:\>setx PYSPARK_PYTHON "C:\Users\User\AppData\Local\Programs\Python\Python39\python.exe"

SUCCESS: Specified value was saved.

C:\>pyspark
Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/19 19:30:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.4
      /_/

Using Python version 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022 16:36:42)
Spark context Web UI available at http://192.168.1.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1752917413213).
SparkSession available as 'spark'.
>>>

%-------------------------------- Download and Installed Python IDE - PyCharm -----------------------------%

https://www.jetbrains.com/pycharm/download/?section=windows

## Download the community edition

%-------------------------------First Application Program ------------------------------------%

from pyspark.sql import *

if __name__ == "__main__":
    print("Hello PySpark Projects")

    spark = SparkSession.builder\
            .appName("First Spark DataList") \
            .master("local[2]") \
            .getOrCreate()

    data_list = [("Ram", 31),
                 ("Sunil", 45),
                 ("James", 56)]

    df = spark.createDataFrame(data_list).toDF("Name", "Age")
    df.show()
	
	
--------------------------------------------------------------------------------------------

%----------------------------------------- Section-3 Getting Started with Apache Spark ------------------------------------%

%------------------------ First Project ------------------------%

https://opendatacommons.org/licenses/pddl/summary/

## You are free:

- To share: To copy, distribute and use the database.
- To create: To produce works from the database.
- To adapt: To modify, transform and build upon the database.

## Task;
- How to load data into Spark
- How to query data in Spark

## Two Approaches
- Using Spark SQL
- Using Spark data frame API


## Note: Spark create runtime metadata catalog (Data Frame Schema) and remove it after executing query.


## Data Source:

https://catalog.data.gov/dataset/?res_format=CSV

https://spark.apache.org/docs/latest/sql-programming-guide.html

https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html

https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html


/Volumes/workspace/default/mydatavolume/sf-fire-calls.csv

%-------------------------------02-spark-dataframe-demo-------------------------------%
 
 fire_df =  spark.read \
			.format('csv') \
			.option('inferSchema', True) \
			.option('header', True) \
			.load(f'/Volumes/workspace/default/mydatavolume/sf-fire-calls.csv')

#fire_df.createGlobalTempView("fire_service_calls_view")

fire_df.createOrReplaceTempView("fire_service_calls_view")


%sql

select * from fire_service_calls_view

---------------------------------------------Creating Sparks Table (03-spark-table-demo) --------------------------------------------
%sql

create database if not exists demo_database

create table if not exists demo_database.fire_service_calls_tbl(
	CallNumber integer,
	UnitID string,
	IncidentNumber integer,
	CallType string,
	CallDate string,
	WatchDate string,
	CallFinalDisposition string,
	AvailableDtTm string,
	Address string,
	City string,
	Zipcode integer,
	Battalion string,
	StationArea string,
	Box string,
	OriginalPriority string,
	Priority string,
	FinalPriority integer,
	ALSUnit boolean,
	CallTypeGroup string,
	NumAlarms integer,
	UnitType string,
	UnitSequenceInCallDispatch integer,
	FirePreventionDistrict string,
	SupervisorDistrict string,
	Neighborhood string,
	Location string,
	RowID string,
	Delay float
) using delta


insert into demo_database.fire_service_calls_tbl
values(1234,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null)


select * from demo_database.fire_service_calls_tbl

## Clean the data in table

truncate table demo_database.fire_service_calls_tbl

insert into demo_database.fire_service_calls_tbl
select * from fire_service_calls_view

INSERT INTO demo_database.fire_service_calls_tbl (CallNumber,UnitID,IncidentNumber,CallType,CallDate,WatchDate,CallFinalDisposition,AvailableDtTm,Address,City,Zipcode,Battalion,StationArea,Box,OriginalPriority,Priority,FinalPriority,ALSUnit,CallTypeGroup,NumAlarms,UnitType,UnitSequenceInCallDispatch,FirePreventionDistrict,SupervisorDistrict,Neighborhood,Location,RowID,Delay)
SELECT CallNumber,UnitID,IncidentNumber,CallType,CallDate,WatchDate,CallFinalDisposition,AvailableDtTm,Address,City,Zipcode,Battalion,StationArea,Box,OriginalPriority,Priority,FinalPriority,ALSUnit,CallTypeGroup,NumAlarms,UnitType,UnitSequenceInCallDispatch,FirePreventionDistrict,SupervisorDistrict,Neighborhood,Location,RowID,Delay
FROM fire_service_calls_view

----------------------------------------------------------------------------------------------------------------

%sql
INSERT INTO demo_database.fire_service_calls_tbl (
    CallNumber,
    UnitID,
    IncidentNumber,
    CallType,
    CallDate,
    WatchDate,
    CallFinalDisposition,
    AvailableDtTm,
    Address,
    City,
    Zipcode,
    Battalion,
    StationArea,
    Box,
    OriginalPriority,
    Priority,
    FinalPriority,
    ALSUnit,
    CallTypeGroup,
    NumAlarms,
    UnitType,
    UnitSequenceInCallDispatch,
    FirePreventionDistrict,
    SupervisorDistrict,
    Neighborhood,
    Location,
    RowID,
    Delay
)
SELECT 
    CallNumber,
    UnitID,
    IncidentNumber,
    CallType,
    CallDate,
    WatchDate,
    CallFinalDisposition,
    AvailableDtTm,
    Address,
    City,
    Zipcode,
    Battalion,
    StationArea,
    Box,
    OriginalPriority,
    Priority,
    FinalPriority,
    ALSUnit,
    CallTypeGroup,
    NumAlarms,
    UnitType,
    UnitSequenceInCallDispatch,
    FirePreventionDistrict,
    SupervisorDistrict,
    Neighborhood,
    Location,
    RowID,
    Delay
FROM fire_service_calls_view


---------------------------------------------------------

%sql
select * from default.sf_fire_calls

----------------------------------------------------------------
%sql

select count(*) default.sf-fire-calls;

%md
###### Q1. How many distinct types of calls were made to the Fire Department?

select count(distinct callType) as distinct_call_type_count
from default.sf-fire-calls
where callType is not null

%md
###### Q2. What were (list) distinct types of calls made to the Fire Department?

select distinct callType as distinct_call_types
from default.sf-fire-calls
where callType is not null

%md
###### Q3. Find out all response for delay times greater the 5 mins?

select CallNumber, Delay
from default.sf-fire-calls
where Dealy > 5

%md
###### Q4. What were the most common call types?
%sql

select callType, count(*) as count
from default.sf-fire-calls
where callType is not null
group by callType
order by count desc

%md
###### Q5. What zip codes accounted for most common calls?
%sql

select callType, Zipcode, count(*) as count
from default.sf-fire-calls
where callType is not null
group by callType, Zipcode
order by count desc


%md
###### Q6. What San Francisco neighborhoods are in the zip codes 94102 and 94103?
%sql

select Zipcode, neighborhood
from default.sf-fire-calls
where Zipcode==94102 or Zipcode==94103



%md
###### Q7. What  was the sum of all calls, average, min and max of the call response times?
%sql

select sum(NumAlarms), avg(Delay), min(Delay), max(Delay)
from default.sf-fire-calls



%md
###### Q8. How many distinct years of data are in the CSV file?
%sql

select distinct year(to_date(CallDate,"yyyy/mm/dd")) as year_num
from default.sf-fire-calls
order by year_num


%md
###### Q9. What  week  of the year in 2002 had the most fire calls?
%sql

select weekofyear(to_date(CallDate, "yyyy/mm/dd")) week_year, count(*) as count
from default.sf-fire-calls
where year(to_date(CallDate,"yyyy/mm/dd")) == 2002
group by week_year
order by count desc

%md
###### Q10. What neighborhoods in San Francisco had the worst response time in 2002?
%sql

select Neighborhood, Delay
from default.sf-fire-calls
where year(to_date(CallDate,"yyyy/mm/dd"))==2002
order by Delay desc

----------------------------------------------------------------------------------------------------
## Dataframe and Transformations Actions

## Applying Transformations

%------------------------------05-working-with-dataframe---------------------------%

fire_df =  spark.read \
			.format('csv') \
			.option('inferSchema', True) \
			.option('header', True) \
			.load(f'/Volumes/workspace/default/mydatavolume/sf-fire-calls.csv')


display(fire_df)

fire_df.printSchema()

root
 |-- CallNumber: integer (nullable = true)
 |-- UnitID: string (nullable = true)
 |-- IncidentNumber: integer (nullable = true)
 |-- CallType: string (nullable = true)
 |-- CallDate: date (nullable = true)
 |-- WatchDate: date (nullable = true)
 |-- CallFinalDisposition: string (nullable = true)
 |-- AvailableDtTm: string (nullable = true)
 |-- Address: string (nullable = true)
 |-- City: string (nullable = true)
 |-- Zipcode: string (nullable = true)
 |-- Battalion: string (nullable = true)
 |-- StationArea: integer (nullable = true)
 |-- Box: integer (nullable = true)
 |-- OriginalPriority: string (nullable = true)
 |-- Priority: string (nullable = true)
 |-- FinalPriority: integer (nullable = true)
 |-- ALSUnit: boolean (nullable = true)
 |-- CallTypeGroup: string (nullable = true)
 |-- NumAlarms: integer (nullable = true)
 |-- UnitType: string (nullable = true)
 |-- UnitSequenceInCallDispatch: integer (nullable = true)
 |-- FirePreventionDistrict: string (nullable = true)
 |-- SupervisorDistrict: string (nullable = true)
 |-- Neighborhood: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- RowID: string (nullable = true)
 |-- Delay: double (nullable = true)

from pyspark.sql.functions import *

fire_df = fire_df.withColumn("Delay", round("Delay",2))

fire_df = fire_df \
	.withColumn("AvailableDtTm", to_timestamp("AvailableDtTm","yyyy/mm/dd hh:mm:ss a"))
	.withColumn("Delay", round("Delay",2))
	

from pyspark.sql.functions import to_timestamp, try_to_timestamp, col
from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

spark = SparkSession.builder.getOrCreate()


fire_df = fire_df.withColumn("AvailableDtTm", to_timestamp(col("AvailableDtTm"), "MM/dd/yyyy hh:mm:ss a"))

fire_df = fire_df.withColumn(
    "AvailableDtTm",
    expr("try_to_timestamp(AvailableDtTm, 'MM/dd/yyyy hh:mm:ss a')")
)

-------------------------------------------------------------------------------------

## Querying Spark Dataframe
- SQL Approach
- Dataframe Transformation Approach

## SQL Approach;
Step-1: Convert your dataframe to temp view
Step-2: Run your SQL on the view

------------------------------------------------------------------------------------


fire_df =  spark.read \
			.format('csv') \
			.option('inferSchema', True) \
			.option('header', True) \
			.load(f'/Volumes/workspace/default/mydatavolume/sf-fire-calls.csv')



%md
###### Q1. How many distinct types of calls were made to the Fire Department?

fire_df.createOrReplaceTempView("fire_service_calls_view")
ql_sql_df = spark.sql( """
	select count(distinct CallType) as distinct_call_type_count
	from fire_service_calls_view
	where CallType is not null
	""")

display(ql_sql_df)
-----------------------------
ql_df = fire_df.where("CallType  is not null").select("CallType").distinct()   ## This dataframe tranformation;
		
print(ql_df.count())  ## This is dataframe action;

## Alternate way to write the above script is below;

ql_df1 = fire_df.where("CallType is  not null")
ql_df2 = ql_df1.select("CallType")
ql_df3 = ql_df2.distinct()
print(ql_df3.count())


%md
###### Q2. What were (list) distinct types of calls made to the Fire Department?

q2_df = fire_df.where("CallType is not null").select(expr("CallType as distinct_call_type")).distinct()

q2_df.show()
display(q2_df)


%md
###### Q3. Find out all response for delay times greater the 5 mins?

fire_df.where("Delay > 5").select("CallNumber","Delay").show()


%md
###### Q4. What were the most common call types?

fire_df.select("CallType").where("CallType is not null").groupBy("CallType").count().orderBy("count", ascending=False).show()


%md
###### Q5. What zip codes accounted for most common calls?




%md
###### Q6. What San Francisco neighborhoods are in the zip codes 94102 and 94103?


%md
###### Q7. What  was the sum of all calls, average, min and max of the call response times?



%md
###### Q8. How many distinct years of data are in the CSV file?



%md
###### Q9. What  week  of the year in 2002 had the most fire calls?


%md
###### Q10. What neighborhoods in San Francisco had the worst response time in 2002?



%----------------------------------------- Section-4 Spark Execution Model and Architecture -------------------------------%
1. How to Create Spark Programs?
2. How to execute Spark Programs?

## How to execute Spark Programs?
- Interactive Clients. (spark-shell, Notebook)
- Submit Job (Spark Submit, Databrick Notebooks, Rest API)

## Spark Distributed Processing Model - How programs run?
- Follows the Master-Slave Architecture (Client-Server)

## Spark Execution Modes and Cluster Managers
1. Client
2. Cluster

------------------------------------------------------------------------------------------

C:\Users\User> pyspark --master local[3] --driver-memory 2G
Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/07/23 10:53:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.4
      /_/

Using Python version 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022 16:36:42)
Spark context Web UI available at http://10.20.22.213:4040
Spark context available as 'sc' (master = local[3], app id = local-1753232020344).
SparkSession available as 'spark'.

>>> df = spark.read.json("C:/Udemy_Course/ApacheSparkePython/Projects/data/people.json")
>>> df.show()
+----+--------+
| age|    name|
+----+--------+
|  51|Prashant|
|  30|   Abdul|
|  19|  Justin|
|  43|    Andy|
+----+--------+

>>> 

------------------------------------------------------------------------------------
## Installing Multi-Node Spark Cluster - Demo
- Creating Cluster in Google Cloud;


%----------------------------------------- Section-5 Spark Programming Model and Developer Experience ---------------------%
## Creating Spark Project Build Configuration

------------------------- FirstDemoProgram.py -----------------------
from pyspark.sql import *


if __name__= "__main__":
	print("Starting First PySpark Program.")


---------------------------------------------------------------------
## Configuring Spark Project Application Logs

C:\> echo %SPARK_HOME%

C:\Sparks\spark-3.4.4

## Sparks default config pages;

C:\Sparks\spark-3.4.4\conf

%------------------------------------------------- spark-defaults.conf -----------------------------------------------%

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.driver.extraJavaOptions	   -Dlog4j.configuration=file:log4j.properties -Dspark.yarn.app.container.log.dir=app-logs -Dlogfile.name=demo-spark

%----------------------------------------------------------------------------------------------------------------------%
## Creating Spark Session

------------------------- FirstDemoProgram.py -----------------------
from pyspark.sql import *
from lib.logger import Log4J

if __name__= "__main__":
	print("Starting First PySpark Program.")
	spark = SparkSession.builder \
		.appName("Demo Spark") \
		.master("local[3]") \
		.getOrCreate()
		
	sc = spark.sparkContext
	sc.setLogLevel("INFO")

	logger = Log4J(spark)
	
	logger.info("Starting DemoSpark")
	
	logger.info("Finished DemoSpark")
	#spark.stop()
	
---------------------------------------------------------------------
- Create new python Package (lib)
- Inside package(lib) create file - logger.py

---------------------------logger.py---------------------------
class Log4J:
	def __init__(self, spark):
		log4j = spark._jvm.org.apache.log4j
		root_class = "guru.learningjournal.spark.examples"
		conf = spark.sparkContext.getConf()
		app_name = conf.get("spark.app.name")
		self.logger = log4j.LogManager.getLogger( root_class+ "." + app_name)
		
	
	def warn(self, message):
		self.logger.warn(message)
		
	def info(self, message):
		self.logger.info(message)
		
	def error(self, message):
		self.logger.error(message)
	
	def debug(self, message):
		self.logger.debug(message)
		
	
---------------------------------------------------------------

## Configuring Spark Session

>> C:\Sparks\spark-3.4.4> bin\spark-submit --master local[3] --conf "spark.app.name=Demo Spark" --conf spark.eventLog.enabled=false FirstDemoProgram.py


---------------------------------FirstDemoProgram.py-------------------------

from pyspark.sql import *
from lib.logger import Log4J

if __name__ == "__main__":
	#print("Starting First PySpark Program.")
	conf = SparkConf()
	conf.set("spark.app.name", "Demo Spark")
	conf.set("spark.master", "local[3]")
	spark = SparkSession.builder \
		.config(conf=conf)\
		.getOrCreate()
	sc = spark.sparkContext
	sc.setLogLevel("INFO")

	logger = Log4J(spark)

	logger.info("Starting DemoSpark")

	logger.info("Finished DemoSpark")


	#spark.stop()
	
	
---------------------------------------------------------------------------------
--------------------------- utils.py ----------------------------
import configparser

from pyspark import SparkConf


def get_spark_app_config():
	spark_conf = SparkConf()
	config = configparser.ConfigParser()
	config.read("spark.conf")
	
	for (key, val) in config.items("SPARK_APP_CONFIGS"):
		spark_conf.set(key, val)
		
	return spark_conf
------------------------------------------------------------------

## Data Frame Introduction

------------------------------- FirstDemoProgram.py ------------------------------
import sys
from pyspark import SparkConf
from pyspark.sql import *
from lib.logger import Log4J
from lib.utils import get_spark_app_config, load_survey_df

if __name__ == "__main__":
	#print("Starting First PySpark Program.")
	#conf = SparkConf()
	conf = get_spark_app_config()
	#conf.set("spark.app.name", "Demo Spark")
	#conf.set("spark.master", "local[3]")
	spark = SparkSession.builder \
		.config(conf=conf)\
		.getOrCreate()
	sc = spark.sparkContext
	sc.setLogLevel("INFO")

	logger = Log4J(spark)

	if len(sys.argv) != 2:
		logger.error("Usage: DemoSpark <filename>")
		sys.exit(-1)

	logger.info("Starting DemoSpark")
	#conf_out = sc.getConf()
	#logger.info(conf_out.toDebugString())

	#survey_df = spark.read \
	#			 .option("header", "true") \
	#			 .option("inferSchema", "true") \
	#			 .csv(sys.argv[1])

	survey_df = load_survey_df(spark, sys.argv[1])
	survey_df.show()
	logger.info("Finished DemoSpark")


	#spark.stop()

-----------------------------------------------------------------------------------
--------------------------- utils.py ----------------------------
import configparser

from pyspark import SparkConf


def get_spark_app_config():
	spark_conf = SparkConf()
	config = configparser.ConfigParser()
	config.read("spark.conf")
	
	for (key, val) in config.items("SPARK_APP_CONFIGS"):
		spark_conf.set(key, val)
		
	return spark_conf


def load_survey_df(spark, data_file):
    return spark.read \
        .option("header", "true") \
        .option("inferSchema", "true") \
        .csv(data_file)


------------------------------------------------------------------

## Data Frame Partitions and Executors

------------------------------ FirstDemoProgram.py ----------------------------

import sys
from pyspark import SparkConf
from pyspark.sql import *
from lib.logger import Log4J
from lib.utils import get_spark_app_config, load_survey_df, count_by_country

if __name__ == "__main__":
	#print("Starting First PySpark Program.")
	#conf = SparkConf()
	conf = get_spark_app_config()
	#conf.set("spark.app.name", "Demo Spark")
	#conf.set("spark.master", "local[3]")
	spark = SparkSession.builder \
		.config(conf=conf)\
		.getOrCreate()
	sc = spark.sparkContext
	sc.setLogLevel("INFO")

	logger = Log4J(spark)

	if len(sys.argv) != 2:
		logger.error("Usage: DemoSpark <filename>")
		sys.exit(-1)

	logger.info("Starting DemoSpark")
	#conf_out = sc.getConf()
	#logger.info(conf_out.toDebugString())

	#survey_df = spark.read \
	#			 .option("header", "true") \
	#			 .option("inferSchema", "true") \
	#			 .csv(sys.argv[1])

	survey_df = load_survey_df(spark, sys.argv[1])
	#filtered_survey_df = survey_df.where("Age < 40") \
	#		.select("Age", "Gender", "Country", "state") \

	#grouped_df = filtered_survey_df.groupBy("Country")
	#count_df = grouped_df.count()
	partioned_survey_df = survey_df.repartition(2)
	#count_df = count_by_country(survey_df)
	#count_df.show()
	#survey_df.show()
	count_df = count_by_country(partioned_survey_df)
	logger.info(count_df.collect())

	input("Press Enter") # remove this line in production. It is only for debugging purpose
	logger.info("Finished DemoSpark")


	#spark.stop()
------------------------------------------------------------------------------
---------------------------------spark.conf ---------------------------
[SPARK_APP_CONFIGS]
spark.app.name = Demo Spark
spark.master = local[3]
app.author = Jnyananjoy
spark.sql.shuffle.partitions = 2

--------------------------------------------------------------------------

http://localhost:4040/jobs/

## Spark Transformations and Actions



## Spark Jobs Stages and Task


## Understanding your Execution Plan


## Unit Testing Spark Application
-----------------------------------test_utils.py----------------------------------
from unittest import TestCase
from pyspark.sql import SparkSession
from lib.utils import load_survey_df, count_by_country

class UtilsTestCase(TestCase):

    @classmethod
    def setUpClass(cls) -> None:
        cls.spark = SparkSession.builder \
        .master("local[3]") \
        .appName("DemoSparkTest") \
        .getOrCreate()

    def test_datafile_loading(self):
        sample_df = load_survey_df(self.spark, "data/sample.csv")
        result_count = sample_df.count()
        self.assertEqual(result_count,9,"Record count should be 9")

    def test_country_count(self):
        sample_df =load_survey_df(self.spark,"data/sample.csv")
        count_list = count_by_country(sample_df).collect()
        count_dict = dict()
        for row in count_list:
            count_dict[row["Country"]] = row["count"]

        self.assertEqual(count_dict["United States"],4, "Count for Unted State should be 4")
        self.assertEqual(count_dict["Canada"], 2, "Count for Canada should be 2")
        self.assertEqual(count_dict["United Kingdom"], 1, "Count for United Kingdom should be 1")

    #@classmethod
    #def tearDownCLass(cls) -> None:
    #    cls.spark.stop()

-----------------------------------------------------------------------------------

%----------------------------------------- Section-6 Spark Structured API Foundation -------------------------------------%

C:\Users\User>conda env list

# conda environments:
#
base                   C:\Users\User\anaconda3
AIAgentEnv             C:\Users\User\anaconda3\envs\AIAgentEnv
BigDataML              C:\Users\User\anaconda3\envs\BigDataML
FirstSparkDemo         C:\Users\User\anaconda3\envs\FirstSparkDemo
GenAIEnv               C:\Users\User\anaconda3\envs\GenAIEnv
MLDeploymentEnv        C:\Users\User\anaconda3\envs\MLDeploymentEnv


C:\Users\User> conda activate FirstSparkDemo

(FirstSparkDemo) C:\Users\User> pip list
Package    Version
---------- -------
colorama   0.4.6
iniconfig  2.1.0
numpy      2.3.1
packaging  25.0
pip        25.1
pluggy     1.6.0
Pygments   2.19.2
pyspark    3.4.4
pytest     8.4.1
setuptools 78.1.1
wheel      0.45.1

-------------------------------------------------------------------------


- RDD (Resilient Distributed Dataset) - The Resilient Distributed Dataset (RDD) API is a fundamental component of Apache Spark, representing an immutable,
partitioned collection of elements that can be operated on in parallel across a cluster. 
It was the primary user-facing API in Spark's early versionsand serves as the low-level abstraction upon which higher-level APIs like DataFrames and Datasets are built.

- Note: Dataset API - To use the Dataset API we need to use JVM based language such as Java and Scala.

## Spark RDD (Resilient Distributed Dataset) API 

## Spark SQL Engine and Catalyst Optimizer

%----------------------------------------- Section-7 Spark Data Sources and Sinks -----------------------------------------%

## How to use DataFrameReader for CSV,JSON, and Parquet

- Spark Data Types
- How to explicitly define a schema for your data.

## Schema for DataFrame
- Explicit
- Implicit

## Note: Parquet file format is preferred format for Spark DataFrame processing.


## Spark DataFrame Schema
- Spark Data Types

## Spark DataFrameWriter API (Sink API)
- 

## Spark Database and Tables
- Managed Tables (managed both Metadata and Data) - > data storage location in spark sql warehouse directory.
- Unmanaged Tables (External Tables) - specify location directory.

## Spark SQL Tables


%----------------------------------------- Section-8 Spark Dataframe and Dataset Transformations --------------------------%
## Data Transformation
- Combining Data Frames
- Aggregating and Summarizing
- Appying Functions and built-in Transformations
- Using Built-in and column-level functions
- Creating and using UDFs
- Referencing Rows/Columns
- Creating Column Expressions

## Dataframe Rows


## DataFrame Rows and Unit Testing


## DataFrame Rows and Unstructured data


## Working with Dataframe Columns


## Creating and Using UDFs


## Misc Transformations


%----------------------------------------- Section-9 Aggregations in Apache Spark -----------------------------------------%
## Grouping Aggregations



## Windowing Aggregations



%----------------------------------------- Section-10 Spark Dataframe Joins  ----------------------------------------------%

## Dataframe Joins and column name ambiguity

---------------------------------------------------------------------------------------
from pyspark.sql import SparkSession

from lib.logger import Log4J

if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("Grouping Demo") \
        .master("local[2]") \
        .getOrCreate()

    logger = Log4J(spark)

    orders_list = [("01", "02", 350, 1),
                   ("01", "04", 580, 1),
                   ("01", "07", 320, 2),
                   ("02", "03", 450, 1),
                   ("02", "06", 220, 1),
                   ("03", "01", 195, 1),
                   ("04", "09", 270, 3),
                   ("04", "08", 410, 2),
                   ("05", "02", 350, 1)]

    order_df = spark.createDataFrame(orders_list).toDF("order_id", "prod_id", "unit_price", "qty")

    product_list = [("01", "Scroll Mouse", 250, 20),
                    ("02", "Optical Mouse", 350, 20),
                    ("03", "Wireless Mouse", 450, 50),
                    ("04", "Wireless Keyboard", 580, 50),
                    ("05", "Standard Keyboard", 360, 10),
                    ("06", "16 GB Flash Storage", 240, 100),
                    ("07", "32 GB Flash Storage", 320, 50),
                    ("08", "64 GB Flash Storage", 430, 25)]

    product_df = spark.createDataFrame(product_list).toDF("prod_id", "prod_name", "list_price", "qty")

    #product_df.show()
    #order_df.show()

    join_expr = order_df.prod_id == product_df.prod_id

    '''
    order_df.join(product_df, join_expr,"inner") \
        .select("*") \
        .show()
    '''

    product_renamed_df = product_df.withColumnRenamed("qty", "reorder_qty")

    order_df.join(product_renamed_df, join_expr, "inner") \
        .drop(product_renamed_df.prod_id)\
        .select("order_id", "prod_id","prod_name", "unit_price", "qty") \
        .show()

-------------------------------------------------------------------------------------------------------------------

## Outer Joins Dataframe


--------------------------------------------------------------------------------------------------------------

## Internals of Spark Join and shuffle

-----------------------------------------------------------------------
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, monotonically_increasing_id, when, expr
from pyspark.sql.types import *

from lib.logger import Log4J

if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("Shuffle Join Demo") \
        .master("local[2]") \
        .getOrCreate()

    logger = Log4J(spark)

    flight_time_df1 = spark.read.json("data/d1/")
    flight_time_df2 = spark.read.json("data/d2/")

    spark.conf.set("spark.sql.shuffle.partitions",3)

    join_expr = flight_time_df1.id == flight_time_df2.id
    join_df = flight_time_df1.join(flight_time_df2, join_expr, "inner")

    join_df.foreach(lambda f: None)
    input("Press a key to stop....")


-----------------------------------------------------------------------
http://localhost:4040/jobs/


-----------------------------------------------------------------------------------
## Optimizing your joins

- Cut down the size of dataframe.
- Broadcast Join can eliminant shuffle.

http://localhost:4040/SQL/execution/?id=2
----------------------------------------------------------------------------------------------------------------
## Implementing Bucket Joins

---------------------------------------------------------------------------------------

from pyspark.sql import SparkSession


from lib.logger import Log4J

if __name__ == "__main__":
    spark = SparkSession \
        .builder \
        .appName("Bucket Join Demo") \
        .master("local[3]") \
        .enableHiveSupport() \
        .getOrCreate()

    logger = Log4J(spark)

    df1 = spark.read.json("data/d1/")
    df2 = spark.read.json("data/d2/")
    #df1.show()
    #df2.show()

    spark.sql("CREATE DATABASE IF NOT EXISTS MY_DB")
    spark.sql("USE MY_DB")

    '''
    df1.coalesce(1).write \
        .bucketBy(3, "id") \
        .saveAsTable("MY_DB.flight_data1")

    df2.coalesce(1).write \
        .bucketBy(3, "id") \
        .saveAsTable("MY_DB.flight_data2")
    '''

    df3 = spark.read.table("MY_DB.flight_data1")
    df4 = spark.read.table("MY_DB.flight_data2")

    spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)  # to avoid the by default broadcast join.

    join_expr = df3.id == df4.id
    join_df = df3.join(df4, join_expr, "inner")

    join_df.collect()
    input("Press a key s stop...")
	
---------------------------------------------------------------------------------------------------


http://localhost:4040/SQL/execution/?id=2

%----------------------------------------- Section-11 Capstone Project ----------------------------------------------------%
## Project Note: Requirements (High Level)

1. We have 500+ entities
2. MDM platform exports all the entities to the Hadoop platform daily.
3. It is full data load every day.
4. We export complete entity data daily and maintain 7 days of exports.
5. The data export is available in Hive tables partitioned by the load_date.
6. You can take the data for any load_date from the Hive tables.

## Details of requirements;
1. We have entity data available in the Hive tables. The data ingestion pipeline is already in place.
2. We are supposed to develop a Spark application that does the following;
	- Take load date as an input argument
	- Read entity data from the hive tables for the given load date.
	- Process and prepare data according to the requirements
	- Sends prepared entity to Kafka topic.
3. Follow best practices, such as modular  design, code reuse, unit testing , etc.
4. Estimate computation requirements, such as the number of executors, CPU core, memory, etc.
5. Work with the DevOps team to define a CI/CD pipeline.

---------------------------------------------------------------------------------------------------------------------------
## Design and Architecture structure - CI/CD Pipeline;

- pipfile
- .gitignore
- test_data :- accounts, parties, party_address, results
- test_pytest_sbdl.py
- sbdl_main.py
- lib :- __init__.py, Utiles.py, logger.py
- log4j.properties
- conf :- sbdl.conf, spark.conf
- sbdl_submit.sh

## Need to install python 3.10.4 in local machine.

C:\>python --version
Python 3.10.4

C:\>pip --version
pip 22.0.4 from C:\Users\User\AppData\Local\Programs\Python\Python310\lib\site-packages\pip (python 3.10)

C:\>pip install pipenv

C:\>py -m site --user-site
C:\Users\User\AppData\Roaming\Python\Python310\site-packages

C:\>py -m site --user-site

## Set up your path variable for env

C:\>setx PATH "%PATH%;C:\Users\User\AppData\Roaming\Python\Python310\Scripts"

WARNING: The data being saved is truncated to 1024 characters.

SUCCESS: Specified value was saved.

C:\Users\User> pipenv --version
pipenv, version 2025.0.4

C:\Udemy_Course\ApacheSparkePython\Projects\SBDL\.venv\Scripts> activate
 

--------------------------------------------------------------------------------------------------------------------------
git init
git add README.md
git commit -m "first commit"
git branch -M master
git remote add origin https://github.com/jnybrahma/PySparkProject.git
git push -u origin master

----------------------------------------------------------------------------------------------------------------

User@USER-OKH8S5ID5J MINGW64 /c/Udemy_Course/ApacheSparkePython/Projects/SBDL (master)
$ git fetch
From https://github.com/jnybrahma/PySparkProject
 * [new branch]      dev              -> origin/dev
 * [new branch]      feature-change-1 -> origin/feature-change-1
 * [new branch]      release          -> origin/release

User@USER-OKH8S5ID5J MINGW64 /c/Udemy_Course/ApacheSparkePython/Projects/SBDL (master)
$ git fetch origin

User@USER-OKH8S5ID5J MINGW64 /c/Udemy_Course/ApacheSparkePython/Projects/SBDL (master)
$ git branch -a
* master
  remotes/origin/dev
  remotes/origin/feature-change-1
  remotes/origin/master
  remotes/origin/release
  
User@USER-OKH8S5ID5J MINGW64 /c/Udemy_Course/ApacheSparkePython/Projects/SBDL (master)
$ git checkout feature-change-1
branch 'feature-change-1' set up to track 'origin/feature-change-1'.
Switched to a new branch 'feature-change-1'

User@USER-OKH8S5ID5J MINGW64 /c/Udemy_Course/ApacheSparkePython/Projects/SBDL (feature-change-1)

PS C:\Udemy_Course\ApacheSparkePython\Projects\SBDL> git status
On branch feature-change-1
Your branch is up to date with 'origin/feature-change-1'.
---------------------------------------------------------------------------------------------------

## Project CI/CD Pipeline;
- Here we will be using open source Jenkins automation tool for CI/CD pipeline.
- Latest Jenkins tools need JDK-17 or JDK-21
- Install the JDK-21 and Jenkins tools server
- Check that Jenkins service is running in Task Manager --> Services.
- Run the Jenkins tool web browser -http://localhost:8080/
- Installed the necessary plugins for Jenkins server.

## Now in Jenkins Server Click
- Manage Jenkins --> System
- System (Scroll down) - GitHub --> Add Github Server
- Added GitHub Personal Access Token (Need to create personal access token in Github) and add to Secret Text (Kind) in Jenkins.
- In GitHub Project repository (PySparkProject) --> Settings --> Webhooks --> add new webhook (Note: Not working with localhost server)
- In Jenkins --> Create New Item --> (PySpark-Project) choose option --> Multibranch Pipeline
- Git hub repo url - https://github.com/jnybrahma/PySparkProject.git
- Now create new feature branch >> git checkout -b feature-change-2
PS C:\Udemy_Course\ApacheSparkePython\Projects\SBDL> git status
On branch feature-change-1
Your branch is up to date with 'origin/feature-change-1'.

PS C:\Udemy_Course\ApacheSparkePython\Projects\SBDL> git checkout -b feature-change-2

PS C:\Udemy_Course\ApacheSparkePython\Projects\SBDL> git status
On branch feature-change-2
nothing to commit, working tree clean
PS C:\Udemy_Course\ApacheSparkePython\Projects\SBDL>
PS C:\Udemy_Course\ApacheSparkePython\Projects\SBDL>  git push --set-upstream origin feature-change-2


##Note: PyTest file name should start with test_ or end with _test.py that's pytest rule.
##Note: Test function name must start from test_.... that's pytest rule

## KAFKA in Cloud
- Create new account in https://www.confluent.io/get-started/ for kafka in cloud


## Estimating resources for application in production environment
- spark.driver.cores
- spark.driver.memory
- spark.driver.memoryOverhead



%----------------------------------------- Section-12 Keep Learning ----------------------------------------------------%





%----------------------------------------- Section-13 Archived - Apache Spark Introduction -----------------------------------------------%





%----------------------------------------- Section-14 Archived - Installing and Using Apache Spark ---------------------------------------------------%
